#!/usr/bin/env python3
"""
åŠ è½½QLoRAå¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†
æ”¯æŒä¸¤ç§åŠ è½½æ–¹å¼ï¼š
1. åŠ è½½LoRAé€‚é…å™¨ + åŸºç¡€æ¨¡å‹
2. ç›´æ¥åŠ è½½åˆå¹¶åçš„å®Œæ•´æ¨¡å‹
"""

import torch
from transformers import (
    AutoModelForCausalLM, 
    PreTrainedTokenizerFast,
    BitsAndBytesConfig
)
from peft import PeftModel
import argparse


class ModelLoader:
    def __init__(self, tokenizer_path="../tokenizer/byte_level_bpe_tokenizer_v1.json"):
        self.tokenizer_path = tokenizer_path
        self.tokenizer = None
        self.model = None
        
    def load_tokenizer(self):
        """åŠ è½½åˆ†è¯å™¨"""
        print(f"åŠ è½½åˆ†è¯å™¨: {self.tokenizer_path}")
        self.tokenizer = PreTrainedTokenizerFast(
            tokenizer_file=self.tokenizer_path,
            bos_token="<s>", 
            pad_token="<pad>", 
            eos_token="</s>", 
            unk_token="<unk>",
        )
        return self.tokenizer
    
    def load_merged_model(self, merged_model_path="./sft_output_merged"):
        """
        æ–¹æ³•1: åŠ è½½åˆå¹¶åçš„å®Œæ•´æ¨¡å‹ (æ¨è)
        ä¼˜ç‚¹: ç®€å•ç›´æ¥ï¼Œæ— éœ€é‡åŒ–é…ç½®
        ç¼ºç‚¹: æ¨¡å‹æ–‡ä»¶è¾ƒå¤§
        """
        print(f"åŠ è½½åˆå¹¶åçš„å®Œæ•´æ¨¡å‹: {merged_model_path}")
        
        self.model = AutoModelForCausalLM.from_pretrained(
            merged_model_path,
            torch_dtype=torch.float16,
            device_map="auto",  # è‡ªåŠ¨åˆ†é…è®¾å¤‡
            trust_remote_code=True,
        )
        
        if self.tokenizer is None:
            self.load_tokenizer()
            
        print("âœ… åˆå¹¶æ¨¡å‹åŠ è½½å®Œæˆ")
        return self.model
    
    def load_lora_model(self, 
                       base_model_path="../output", 
                       lora_adapter_path="./sft_output",
                       use_4bit=True):
        """
        æ–¹æ³•2: åŠ è½½LoRAé€‚é…å™¨ + åŸºç¡€æ¨¡å‹
        ä¼˜ç‚¹: èŠ‚çœå­˜å‚¨ç©ºé—´ï¼Œå¯ä»¥åˆ‡æ¢ä¸åŒçš„LoRAé€‚é…å™¨
        ç¼ºç‚¹: éœ€è¦åŸºç¡€æ¨¡å‹å’Œé‡åŒ–é…ç½®
        """
        print(f"åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
        print(f"åŠ è½½LoRAé€‚é…å™¨: {lora_adapter_path}")
        
        if use_4bit:
            # 4bité‡åŒ–é…ç½® (ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´)
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
            )
            
            # åŠ è½½é‡åŒ–çš„åŸºç¡€æ¨¡å‹
            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_path,
                quantization_config=bnb_config,
                device_map="auto",
                trust_remote_code=True,
                torch_dtype=torch.float16,
            )
        else:
            # ä¸ä½¿ç”¨é‡åŒ–
            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
            )
        
        # åŠ è½½LoRAé€‚é…å™¨
        self.model = PeftModel.from_pretrained(
            base_model, 
            lora_adapter_path,
            torch_dtype=torch.float16,
        )
        
        if self.tokenizer is None:
            self.load_tokenizer()
            
        print("âœ… LoRAæ¨¡å‹åŠ è½½å®Œæˆ")
        return self.model
    
    def generate_text(self, prompt, max_length=512, temperature=0.7, top_p=0.9):
        """ç”Ÿæˆæ–‡æœ¬ - æ”¹è¿›ç‰ˆï¼Œé¿å…é‡å¤è¾“å‡º"""
        if self.model is None or self.tokenizer is None:
            raise ValueError("è¯·å…ˆåŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨")
        
        # ç¼–ç è¾“å…¥
        inputs = self.tokenizer.encode(prompt, return_tensors="pt")
        inputs = inputs.to(self.model.device)
        
        # æ”¹è¿›çš„ç”Ÿæˆå‚æ•°ï¼Œé¿å…é‡å¤
        generation_config = {
            "max_new_tokens": 256,  # ğŸ”§ ä½¿ç”¨max_new_tokensè€Œä¸æ˜¯max_length
            "min_length": len(inputs[0]) + 10,  # ğŸ”§ è®¾ç½®æœ€å°é•¿åº¦
            "temperature": temperature,
            "top_p": top_p,
            "top_k": 50,  # ğŸ”§ æ·»åŠ top_ké™åˆ¶
            "do_sample": True,
            "pad_token_id": self.tokenizer.pad_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
            "repetition_penalty": 1.1,  # ğŸ”§ æ·»åŠ é‡å¤æƒ©ç½š
            "no_repeat_ngram_size": 3,  # ğŸ”§ é¿å…3-gramé‡å¤
        }
        
        # ç”Ÿæˆæ–‡æœ¬
        with torch.no_grad():
            outputs = self.model.generate(inputs, **generation_config)
        
        # è§£ç è¾“å‡º
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # å»é™¤è¾“å…¥éƒ¨åˆ†ï¼Œåªè¿”å›ç”Ÿæˆçš„éƒ¨åˆ†
        generated_only = generated_text[len(prompt):].strip()
        return generated_only


def main():
    parser = argparse.ArgumentParser(description="åŠ è½½QLoRAå¾®è°ƒæ¨¡å‹è¿›è¡Œæ¨ç†")
    parser.add_argument("--mode", choices=["merged", "lora"], default="merged",
                       help="åŠ è½½æ¨¡å¼: merged(åˆå¹¶æ¨¡å‹) æˆ– lora(LoRAé€‚é…å™¨)")
    parser.add_argument("--merged_path", default="./sft_output_merged",
                       help="åˆå¹¶æ¨¡å‹è·¯å¾„")
    parser.add_argument("--base_model", default="../output",
                       help="åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--lora_path", default="./sft_output",
                       help="LoRAé€‚é…å™¨è·¯å¾„")
    parser.add_argument("--no_4bit", action="store_true",
                       help="ä¸ä½¿ç”¨4bité‡åŒ– (ä»…å¯¹LoRAæ¨¡å¼æœ‰æ•ˆ)")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–åŠ è½½å™¨
    loader = ModelLoader()
    
    try:
        if args.mode == "merged":
            # æ–¹æ³•1: åŠ è½½åˆå¹¶åçš„å®Œæ•´æ¨¡å‹
            loader.load_merged_model(args.merged_path)
            
        elif args.mode == "lora":
            # æ–¹æ³•2: åŠ è½½LoRAé€‚é…å™¨
            use_4bit = not args.no_4bit
            loader.load_lora_model(
                base_model_path=args.base_model,
                lora_adapter_path=args.lora_path,
                use_4bit=use_4bit
            )
        
        # äº¤äº’å¼å¯¹è¯
        print("\n" + "="*50)
        print("ğŸ¤– æ¨¡å‹åŠ è½½å®Œæˆï¼å¼€å§‹å¯¹è¯ (è¾“å…¥ 'quit' é€€å‡º)")
        print("="*50)
        
        while True:
            user_input = input("\nğŸ‘¤ User: ").strip()
            if user_input.lower() in ['quit', 'exit', 'q']:
                break
            
            if not user_input:
                continue
            
            # æ ¼å¼åŒ–ä¸ºè®­ç»ƒæ—¶çš„æ ¼å¼
            prompt = f"User: {user_input}\nAssistant:"
            
            # ç”Ÿæˆå›å¤
            print("ğŸ¤– Assistant: ", end="", flush=True)
            response = loader.generate_text(
                prompt, 
                max_length=512, 
                temperature=0.7, 
                top_p=0.9
            )
            print(response)
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ å¯¹è¯ç»“æŸ")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")


if __name__ == "__main__":
    main()
