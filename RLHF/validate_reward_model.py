#!/usr/bin/env python3
# å¥–åŠ±æ¨¡å‹éªŒè¯è„šæœ¬
# 2025/9/10
# wenzhe

import os
import torch
from transformers import (
    AutoModelForSequenceClassification,
    PreTrainedTokenizerFast,
)

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['HF_DATASETS_CACHE'] = '/data/wenzhe/huggingface/datasets'
os.environ['HF_HOME'] = '/data/wenzhe/huggingface'


def load_reward_model(reward_model_path="./reward_model_output", tokenizer_path="../tokenizer/byte_level_bpe_tokenizer_v1.json"):
    """åŠ è½½å®Œæ•´çš„å¥–åŠ±æ¨¡å‹"""
    print("ğŸ”„ åŠ è½½åˆ†è¯å™¨...")
    tokenizer = PreTrainedTokenizerFast(
        tokenizer_file=tokenizer_path,
        bos_token="<s>", 
        pad_token="<pad>", 
        eos_token="</s>", 
        unk_token="<unk>",
    )
    tokenizer.pad_token = tokenizer.eos_token
    
    print("ğŸ”„ åŠ è½½å®Œæ•´å¥–åŠ±æ¨¡å‹...")
    # ç›´æ¥åŠ è½½å®Œæ•´çš„å¥–åŠ±æ¨¡å‹ï¼ˆå·²ç»mergeçš„ï¼‰
    model = AutoModelForSequenceClassification.from_pretrained(
        reward_model_path,
        device_map="auto",
        torch_dtype=torch.float16,
    )
    model.eval()
    
    print("âœ… å¥–åŠ±æ¨¡å‹åŠ è½½å®Œæˆï¼")
    return model, tokenizer


def get_reward_score(model, tokenizer, instruction, response):
    """è®¡ç®—å¥–åŠ±åˆ†æ•°"""
    text = f"Instruction: {instruction}\nResponse: {response}"
    
    inputs = tokenizer(
        text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=1024
    ).to(model.device)
    
    with torch.no_grad():
        outputs = model(**inputs)
        reward_score = outputs.logits.squeeze().cpu().item()
    
    return reward_score


def test_reward_model():
    """æµ‹è¯•å¥–åŠ±æ¨¡å‹"""
    try:
        # åŠ è½½æ¨¡å‹
        model, tokenizer = load_reward_model()
        
        # æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            {
                "instruction": "è§£é‡Šä»€ä¹ˆæ˜¯Pythonç¼–ç¨‹è¯­è¨€",
                "good_response": "Pythonæ˜¯ä¸€ç§é«˜çº§ã€è§£é‡Šå‹ç¼–ç¨‹è¯­è¨€ï¼Œä»¥å…¶ç®€æ´æ˜“è¯»çš„è¯­æ³•è‘—ç§°ã€‚å®ƒæ”¯æŒé¢å‘å¯¹è±¡ã€å‡½æ•°å¼å’Œè¿‡ç¨‹å¼ç¼–ç¨‹èŒƒå¼ï¼Œå¹¿æ³›åº”ç”¨äºWebå¼€å‘ã€æ•°æ®ç§‘å­¦ã€äººå·¥æ™ºèƒ½ã€è‡ªåŠ¨åŒ–è„šæœ¬ç­‰é¢†åŸŸã€‚Pythonæ‹¥æœ‰ä¸°å¯Œçš„æ ‡å‡†åº“å’Œç¬¬ä¸‰æ–¹åŒ…ç”Ÿæ€ç³»ç»Ÿã€‚",
                "bad_response": "Pythonå°±æ˜¯è›‡ã€‚"
            },
            {
                "instruction": "å¦‚ä½•å­¦ä¹ æœºå™¨å­¦ä¹ ï¼Ÿ",
                "good_response": "å­¦ä¹ æœºå™¨å­¦ä¹ å»ºè®®æŒ‰ä»¥ä¸‹æ­¥éª¤ï¼š1.æŒæ¡æ•°å­¦åŸºç¡€(çº¿æ€§ä»£æ•°ã€æ¦‚ç‡ç»Ÿè®¡ã€å¾®ç§¯åˆ†)ï¼›2.å­¦ä¹ Pythonç¼–ç¨‹å’Œç›¸å…³åº“(numpyã€pandasã€scikit-learn)ï¼›3.ç†è§£åŸºæœ¬ç®—æ³•åŸç†ï¼›4.é€šè¿‡å®é™…é¡¹ç›®ç»ƒä¹ ï¼›5.å‚åŠ åœ¨çº¿è¯¾ç¨‹å’Œé˜…è¯»ç»å…¸æ•™æï¼›6.å…³æ³¨æœ€æ–°ç ”ç©¶è¿›å±•ã€‚",
                "bad_response": "çœ‹ä¹¦å°±è¡Œäº†ã€‚"
            },
            {
                "instruction": "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
                "good_response": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚é€šè¿‡å¤§é‡æ•°æ®è®­ç»ƒï¼Œæ·±åº¦ç¥ç»ç½‘ç»œèƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ æ•°æ®çš„å±‚æ¬¡åŒ–ç‰¹å¾è¡¨ç¤ºã€‚å®ƒåœ¨è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚",
                "bad_response": "æ·±åº¦å­¦ä¹ å°±æ˜¯å¾ˆæ·±çš„å­¦ä¹ ã€‚"
            }
        ]
        
        print("\nğŸ¯ å¥–åŠ±æ¨¡å‹éªŒè¯ç»“æœï¼š")
        print("=" * 50)
        
        correct_count = 0
        total_count = len(test_cases)
        
        for i, case in enumerate(test_cases, 1):
            print(f"\næµ‹è¯• {i}: {case['instruction']}")
            
            good_score = get_reward_score(model, tokenizer, case['instruction'], case['good_response'])
            bad_score = get_reward_score(model, tokenizer, case['instruction'], case['bad_response'])
            
            print(f"  è¯¦ç»†å›ç­”åˆ†æ•°: {good_score:.4f}")
            print(f"  ç®€å•å›ç­”åˆ†æ•°: {bad_score:.4f}")
            print(f"  åˆ†æ•°å·®å¼‚: {good_score - bad_score:.4f}")
            
            if good_score > bad_score:
                print("  âœ… æ­£ç¡®è¯†åˆ«")
                correct_count += 1
            else:
                print("  âŒ è¯†åˆ«é”™è¯¯")
        
        accuracy = correct_count / total_count
        print(f"\nğŸ“Š æ€»ä½“ç»“æœ:")
        print(f"  æ­£ç¡®è¯†åˆ«: {correct_count}/{total_count}")
        print(f"  å‡†ç¡®ç‡: {accuracy:.2%}")
        
        if accuracy >= 0.8:
            print("  ğŸ‰ å¥–åŠ±æ¨¡å‹è¡¨ç°ä¼˜ç§€ï¼")
        elif accuracy >= 0.6:
            print("  ğŸ‘ å¥–åŠ±æ¨¡å‹è¡¨ç°è‰¯å¥½")
        else:
            print("  âš ï¸  å¥–åŠ±æ¨¡å‹å¯èƒ½éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
        
    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")
        print("\nè¯·æ£€æŸ¥:")
        print("1. å¥–åŠ±æ¨¡å‹è®­ç»ƒæ˜¯å¦å®Œæˆ")
        print("2. å®Œæ•´å¥–åŠ±æ¨¡å‹è·¯å¾„: ./reward_model_output/")
        print("3. åˆ†è¯å™¨è·¯å¾„: ../tokenizer/")


if __name__ == "__main__":
    test_reward_model()
